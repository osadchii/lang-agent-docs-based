# Развертывание (Deployment)































## Окружения































### Локальное окружение















Локальная разработка выполняется с использованием:















- Backend: `uvicorn` в режиме hot-reload















- Frontend: `npm run dev` с Vite dev server















- База данных: PostgreSQL в Docker контейнере















- Redis: Redis в Docker контейнере















- Telegram bot: webhook в режиме polling для локальной разработки































Запуск:















```bash















# Инфраструктура (PostgreSQL + Redis)















docker-compose -f docker-compose.local.yml up -d































# Backend















cd backend















python -m venv venv















source venv/bin/activate















pip install -r requirements.txt















uvicorn app.main:app --reload































# Frontend















cd frontend















npm install















npm run dev















```































**docker-compose.local.yml (пример):**















```yaml















services:















  postgres:















    image: postgres:15















    environment:















      POSTGRES_DB: langagent_local















      POSTGRES_USER: postgres















      POSTGRES_PASSWORD: local_password















    ports:















      - "5432:5432"















    volumes:















      - postgres_local_data:/var/lib/postgresql/data































  redis:















    image: redis:7-alpine















    ports:















      - "6379:6379"















    volumes:















      - redis_local_data:/data































volumes:















  postgres_local_data:















  redis_local_data:















```































Опционально: для локальной отладки можно добавить Grafana и Loki в `docker-compose.local.yml` для просмотра логов и простых дашборд (см. этап C в `to-do.md`).































### Production















Продакшн окружение:















- VPS или облачный сервер (AWS, DigitalOcean, Hetzner и т.д.)















- PostgreSQL база данных















- Docker контейнеры для backend















- Статический хостинг для frontend (Nginx, Vercel, Netlify)















- Telegram bot с webhook для получения обновлений































## Инфраструктура































### Сервер















Рекомендуемая конфигурация:















- **CPU**: 2+ ядра















- **RAM**: 4+ GB















- **Disk**: 50+ GB SSD















- **OS**: Ubuntu 22.04 LTS или выше































Установленное ПО:















- Docker и Docker Compose















- Nginx (reverse proxy)















- Certbot (SSL сертификаты)































### База данных















PostgreSQL 15+:















- Размещение: тот же сервер или managed service (AWS RDS, DigitalOcean Managed DB)















- Настройки:















  - `max_connections`: 100















  - `shared_buffers`: 1GB















  - `work_mem`: 16MB















- Регулярные бэкапы (см. секцию Backup)















- Репликация для высокой доступности (опционально)































### Redis















Redis 7+ для кэширования и управления состоянием:















- **Использование:**















  - Кэширование (hot data: активный профиль, user stats)















  - Session storage (состояния диалогов бота FSM)















  - Rate limiting (счетчики для лимитов API)















  - LLM cache (переводы, лемматизация)















- **Размещение:** Docker контейнер на том же сервере или managed service (AWS ElastiCache, DigitalOcean Managed Redis)















- **Настройки:**















  - `maxmemory`: 512MB - 1GB















  - `maxmemory-policy`: allkeys-lru (вытеснение старых ключей)















  - Persistence: AOF (append-only file) для сохранения данных















- **Monitoring:**















  - Использование памяти (`INFO memory`)















  - Hit rate кэша















  - Количество подключений































**Примечание:** Redis является критичным компонентом для продакшена. Без Redis сервис не будет работать корректно.































### Storage















Хранение файлов (изображения, аудио):















- **Локальное хранение**: `/opt/lang-agent/media` на сервере (для небольших объемов)















- **Объектное хранилище**: MinIO, DigitalOcean Spaces или аналогичный сервис (опционально)















  - Автоматическое резервное копирование















  - CDN интеграция















  - Версионирование файлов































### CDN















Раздача статики фронтенда:















- **Nginx**: локальная раздача с кешированием















- **CloudFlare**: бесплатный CDN с SSL















- **AWS CloudFront**: для глобального распределения















- **Vercel/Netlify**: автоматический CDN при хостинге фронтенда































## Backend deployment































### Docker















Dockerfile для бэкенда (`backend/Dockerfile`) повторяет продовый сценарий запуска:















```dockerfile















FROM python:3.11-slim































ENV PYTHONUNBUFFERED=1 \















    PYTHONDONTWRITEBYTECODE=1































WORKDIR /app































RUN apt-get update \















    && apt-get install --no-install-recommends -y curl build-essential \















    && rm -rf /var/lib/apt/lists/*































COPY requirements.txt .















RUN pip install --upgrade pip && pip install -r requirements.txt































COPY . .































RUN adduser --disabled-password --gecos "" appuser \















    && chown -R appuser:appuser /app \















    && chmod +x /app/docker-entrypoint.sh































USER appuser















ENV PYTHONPATH=/app































ENTRYPOINT ["/app/docker-entrypoint.sh"]















```































Точка входа (`backend/docker-entrypoint.sh`) перед стартом `uvicorn` выполняет миграции:















```bash















#!/usr/bin/env bash















set -euo pipefail































alembic upgrade head































exec uvicorn app.main:app --host "0.0.0.0" --port "${PORT:-8000}"















```































Docker Compose для деплоя (`docker-compose.yml` в корне репозитория):















```yaml















services:















  backend:















    image: "${BACKEND_IMAGE:-ghcr.io/osadchii/lang-agent-docs-based/backend}:${BACKEND_IMAGE_TAG:-latest}"















    ports:















      - "8000:8000"















    env_file:















      - .env















    depends_on:















      db:















        condition: service_healthy















      redis:















        condition: service_healthy















    healthcheck:















      test: ["CMD-SHELL", "curl -fsS http://localhost:8000/health || exit 1"]















      interval: 30s















      timeout: 5s















      retries: 5















- Job deploy внутри .github/workflows/backend-deploy.yml просто копирует compose/infra на сервер и выполняет docker compose pull && docker compose up -d frontend, без ручной загрузки артефактов.















    restart: unless-stopped















    networks:















      - app-network































  db:















    image: postgres:15















    env_file:















      - .env















    volumes:















      - postgres_data:/var/lib/postgresql/data















    healthcheck:















      test:















        [















          "CMD-SHELL",















          "pg_isready -U $$POSTGRES_USER -d $$POSTGRES_DB",















        ]















      interval: 10s















      timeout: 5s















      retries: 5















    restart: unless-stopped















    networks:















      - app-network































  redis:















    image: redis:7-alpine















    command: redis-server --appendonly yes















    volumes:















      - redis_data:/data















    healthcheck:















      test: ["CMD", "redis-cli", "ping"]















      interval: 10s















      timeout: 3s















      retries: 5















    restart: unless-stopped















    networks:















      - app-network































volumes:















  postgres_data:















  redis_data:































networks:















  app-network:















    driver: bridge















```































**Важно**: Docker Compose использует уже собранный образ из GitHub Container Registry (`ghcr.io/osadchii/lang-agent-docs-based/backend`). Для smoke‑тестов по-прежнему можно выполнить `docker compose build backend`, но продакшн сценарий тянет готовые теги командой `docker compose pull backend`.































CI/CD pipeline автоматически копирует актуальный `docker-compose.yml` на сервер в `/opt/lang-agent/docker-compose.yml`, логинится в GHCR и выполняет `docker compose pull && docker compose up -d --remove-orphans` сразу после успешной публикации образа.































### Environment variables































Файл `.env` создается **вручную на сервере** владельцем проекта перед первым деплоем и хранится в `/opt/lang-agent/.env`.































**Содержимое `.env` файла:** (все значения задокументированы в `.env.example`)































| Переменная | Обязательна | Описание | Пример |















| --- | --- | --- | --- |















| `PROJECT_NAME` | нет | Отображаемое имя сервиса | Lang Agent Backend |















| `APP_ENV` | нет | Окружение (`local/test/staging/production`) | production |















| `DEBUG` | нет | Включает swagger и подробные ошибки | false |















| `API_V1_PREFIX` | нет | Префикс REST API | /api |















| `LOG_LEVEL` | нет | Уровень логирования | INFO |















| `DATABASE_URL` | **да** | Подключение к PostgreSQL (`asyncpg`) | `postgresql+asyncpg://...` |















| `REDIS_URL` | **да** | Подключение к Redis | `redis://redis:6379/0` |















| `SECRET_KEY` | **да** | JWT‑секрет (`openssl rand -hex 32`) | `1a2b...` |















| `JWT_ALGORITHM` | нет | Алгоритм подписи токенов | HS256 |















| `ACCESS_TOKEN_EXPIRE_MINUTES` | нет | TTL access токена (мин) | 30 |















| `TELEGRAM_BOT_TOKEN` | **да** | Токен BotFather | `123456:ABC...` |















| `OPENAI_API_KEY` | **да** | Ключ OpenAI | `sk-...` |















| `ANTHROPIC_API_KEY` | нет | Ключ Claude (если используем) | `sk-ant-...` |















| `LLM_MODEL` | нет | Модель по умолчанию | gpt-4.1-mini |















| `LLM_TEMPERATURE` | нет | Творчество LLM (`0..1`) | 0.7 |















| `PRODUCTION_APP_ORIGIN` | нет | Боевой origin Mini App | https://mini.lang-agent.app |















| `BACKEND_DOMAIN` | нет | Публичный backend-домен без схемы (используется nginx-proxy и webhook URL) | backend.external.osadchii.me |















| `BACKEND_CORS_ORIGINS` | нет | Локальный whitelist (только `http://localhost:<port>`, работает при `APP_ENV=local/test`) | http://localhost:4173 |















| `MAX_REQUEST_BYTES` | нет | Лимит тела запроса (байты, default 1 MiB) | 1048576 |
| `RATE_LIMIT_IP_PER_MINUTE` | нет | Количество запросов в минуту с одного IP (для защиты edge) | 100 |
| `RATE_LIMIT_USER_PER_HOUR` | нет | Почасовой лимит для авторизованного пользователя | 1000 |
| `RATE_LIMIT_FREE_LLM_PER_DAY` | нет | Дневной лимит сообщений LLM для бесплатного плана | 50 |
| `RATE_LIMIT_PREMIUM_LLM_PER_DAY` | нет | Дневной лимит сообщений LLM для премиума | 500 |
| `RATE_LIMIT_FREE_EXERCISES_PER_DAY` | нет | Дневной лимит генераций упражнений для бесплатного плана | 10 |
| `RATE_LIMIT_PREMIUM_EXERCISES_PER_DAY` | нет | Дневной лимит упражнений для премиума (0 = без ограничений) | 0 |
| `RATE_LIMIT_WORKER_ENABLED` | нет | Запускать ежедневный воркер сброса счетчиков (только на staging/prod) | false |
| `RATE_LIMIT_RESET_HOUR_UTC` | нет | UTC-час для очистки суточных лимитов | 0 |
| `RATE_LIMIT_RESET_MINUTE_UTC` | нет | UTC-минуты для очистки суточных лимитов | 5 |















| `STRIPE_SECRET_KEY` | нет | Ключ Stripe (подписки) | `sk_live_...` |















| `STRIPE_WEBHOOK_SECRET` | нет | Проверка событий Stripe | `whsec_...` |















| `STRIPE_PRICE_ID_BASIC` / `STRIPE_PRICE_ID_PREMIUM` | нет | ID тарифов | `price_xxx` |















Инфраструктурные переменные (используются Docker Compose и CI/CD):































| Переменная | Назначение |















| --- | --- |















| `BACKEND_IMAGE`, `BACKEND_IMAGE_TAG` | Тег backend‑образа в GHCR |



| FRONTEND_IMAGE, FRONTEND_IMAGE_TAG | Тег frontend-образа в GHCR |















| `POSTGRES_DB`, `POSTGRES_USER`, `POSTGRES_PASSWORD` | Настройки контейнера db |















| `GRAFANA_ADMIN_USER`, `GRAFANA_ADMIN_PASSWORD`, `GRAFANA_DOMAIN`, `TRAEFIK_ACME_EMAIL` | Настройка мониторинга и Let’s Encrypt |































> `nginx-proxy` внутри `docker-compose.yml` автоматически проксирует `https://<BACKEND_DOMAIN>` на backend (порт 8000) и `https://<GRAFANA_DOMAIN>` на Grafana, параллельно запрашивая сертификаты Let's Encrypt. Перед деплоем убедитесь, что оба домена указывают на IP сервера.































**Создание .env файла на сервере:**















```bash















# SSH на сервер















ssh user@your-server































# Создать директорию для приложения















sudo mkdir -p /opt/lang-agent















cd /opt/lang-agent































# Создать .env файл















sudo nano .env















# Вставить переменные окружения и сохранить































# Установить безопасные права доступа















sudo chmod 600 .env















sudo chown $USER:$USER .env















```































**Важно**: `.env` файл НЕ коммитится в репозиторий и НЕ копируется из GitHub Actions. Он создается один раз на сервере и обновляется вручную при необходимости.































### Migrations















Применение миграций при деплое:































1. **Локально**:















```bash















alembic upgrade head















```































2. **В Docker контейнере**:















```bash















docker-compose exec backend alembic upgrade head















```































3. **Автоматически при старте** (добавить в entrypoint.sh):















```bash















#!/bin/bash















# Применение миграций















alembic upgrade head































# Запуск приложения















exec uvicorn app.main:app --host 0.0.0.0 --port 8000















```































4. **В CI/CD pipeline**:















- Миграции применяются автоматически после успешного build















- См. `.github/workflows/deploy.yml`































### Запуск бота















Telegram бот запускается вместе с FastAPI приложением через Docker Compose:































```bash















cd /opt/lang-agent















docker compose up -d















```































Проверка статуса:















```bash















# Посмотреть запущенные контейнеры















docker compose ps































# Логи backend















docker compose logs -f backend































# Логи базы данных















docker compose logs -f db















```































Webhook настраивается автоматически при старте приложения (см. `backend/app/telegram/bot.py`).































## Frontend deployment































### Build















Процесс сборки React приложения:































```bash















cd frontend















npm install















npm run build















```































Результат сборки находится в `frontend/dist/`:















- Оптимизированный JavaScript (минификация, tree-shaking)















- CSS с autoprefixer















- Оптимизированные изображения















- Service Worker для PWA (опционально)















Эта директория упаковывается в Docker-образ (







rontend/Dockerfile) и может использоваться для стороннего деплоя.































### Hosting















Варианты размещения статики:































**0. Docker + nginx-proxy (конфигурация по умолчанию):**















- docker-compose.yml поднимает сервис rontend, использующий образ (FRONTEND_IMAGE) из GHCR (собирается из rontend/Dockerfile) и подключающий infra/frontend/nginx.conf для SPA-fallback.















- `nginx-proxy` + `nginx-acme` автоматически выпускают TLS-сертификаты для домена `FRONTEND_DOMAIN`, поэтому мини-приложение доступно по `https://<FRONTEND_DOMAIN>` сразу после `docker compose up -d frontend`.















- Job deploy внутри .github/workflows/backend-deploy.yml копирует compose/infra на сервер и выполняет docker compose pull && docker compose up -d frontend, артефакт больше не нужен.































**1. Nginx на том же сервере:**















```nginx















server {















    listen 80;















    server_name yourdomain.com;































    root /var/app/frontend/dist;















    index index.html;































    location / {















        try_files $uri $uri/ /index.html;















    }































    location /api {















        proxy_pass http://localhost:8000;















        proxy_set_header Host $host;















        proxy_set_header X-Real-IP $remote_addr;















    }















}















```































**2. Vercel/Netlify:**















- Автоматический деплой из GitHub















- Встроенный CDN















- SSL сертификаты















- Preview deployments для PR































**3. Object storage + CDN:**















- Хранение статики в любом объектном хранилище (MinIO, Spaces и т.п.)















- Раздача через CDN (CloudFront, Cloudflare R2, BunnyCDN)















- Низкая стоимость при высокой географической доступности































### Environment configuration















Переменные окружения для фронтенда настраиваются через `.env` файлы:































- `.env.development` - для локальной разработки















- `.env.production` - для продакшена































`FRONTEND_DOMAIN` обязательно указывает публичный домен Mini App: `docker-compose` подставляет его в `VIRTUAL_HOST/LETSENCRYPT_HOST`, поэтому TLS для фронтенда выпускается автоматически.































При деплое через CI/CD переменные подставляются автоматически из GitHub Secrets.































## Secrets management















Управление секретами (API ключи, токены):































### GitHub Secrets















Секреты для CI/CD хранятся в GitHub Secrets:















- Settings → Secrets and variables → Actions → New repository secret































Необходимые секреты для GitHub Actions:















- `GHCR_USERNAME` - владелец контейнерного реестра (например, `osadchii`)















- `GHCR_TOKEN` - GitHub Personal Access Token с правами `write:packages`















- `SSH_PRIVATE_KEY_LANG_AGENT` - приватный SSH ключ для доступа к серверу деплоя backend















- `SSH_HOST` - IP адрес или домен сервера















- `SSH_PORT` - SSH порт (22 по умолчанию)















- `SSH_USER` - SSH пользователь (например, `ubuntu`)















- `TELEGRAM_DEPLOY_CHAT_ID` - ID чата для уведомлений о деплое/CI (задаётся в GitHub Secrets)















- `CI_BOT_TOKEN` - токен отдельного бота для CI/CD уведомлений (не совпадает с основным `TELEGRAM_BOT_TOKEN`)































**Важно**: Секреты приложения (API ключи, токены БД и т.д.) НЕ хранятся в GitHub Secrets. Они находятся только в `.env` файле на сервере.































### На сервере















Все секреты приложения хранятся в файле `/opt/lang-agent/.env` с ограниченными правами доступа:































```bash















# Установка безопасных прав доступа















chmod 600 /opt/lang-agent/.env















chown $USER:$USER /opt/lang-agent/.env















```































**Безопасность:**















- Файл `.env` доступен только владельцу















- Никогда не коммитьте `.env` в репозиторий















- Добавьте `.env` в `.gitignore`















- Создайте `.env.example` с пустыми значениями для документации































### Ротация секретов















Рекомендации:















- Менять `SECRET_KEY` каждые 90 дней















- API ключи менять при компрометации















- Использовать разные ключи для local/production















- При ротации секретов:















  1. Обновить `.env` файл на сервере















  2. Перезапустить контейнеры: `docker-compose restart backend`































## Мониторинг































### Логирование















Логи хранятся и анализируются:































**Docker logs:**















```bash















docker-compose logs -f backend















docker-compose logs --tail=100 backend















```































**Structured logging** в приложении:















- JSON формат для парсинга















- Уровни: DEBUG, INFO, WARNING, ERROR, CRITICAL















- Ротация логов (logrotate)































**Централизованное логирование:**















- ELK Stack (Elasticsearch, Logstash, Kibana)















- Loki + Grafana















- CloudWatch Logs (AWS)































#### Loki + Grafana stack (production docker-compose)















Наблюдаемость в продовом compose-файле разворачивается штатно вместе с приложением.































1. **Конфиги.** Каталог `infra/` хранит все файлы, которые нужно копировать на сервер рядом с `docker-compose.yml`:















   - `infra/loki/config.yml` — хранение чанк-файлов и ретеншн 7 дней;















   - `infra/promtail/config.yml` — сборщик логов; парсит JSON-поля (`http_method`, `status_code`, `duration_ms`, `request_id`);















   - `infra/grafana/provisioning/...` — datasources и дашборды (RPS, p95 latency, ошибки, top endpoints).















2. **Переменные окружения.** В `.env` добавьте `GRAFANA_ADMIN_USER` и `GRAFANA_ADMIN_PASSWORD` (минимум 16 символов в проде).















3. **Запуск.**















   ```bash















   docker compose up -d backend db redis loki promtail grafana















   ```















   Promtail требует доступа к Docker socket и каталогу `/var/lib/docker/containers`, поэтому команду выполняйте от пользователя с правами на чтение этих путей (обычно `docker` группа).















4. **Доступ.**















   - Grafana доступна по `https://<GRAFANA_DOMAIN>` через связку `nginx-proxy` + `acme-companion`; авторизация — `GRAFANA_ADMIN_*`.















   - Loki доступен только из внутренней сети `app-network`.















   - Первый логин в Grafana использует `GRAFANA_ADMIN_*`; при старте автоматически импортируется datasource `Loki` и дашборд `Backend Observability`.















5. **Nginx + Let's Encrypt.**















   - Контейнер `nginx` (образ `nginxproxy/nginx-proxy`) уже включён в `docker-compose.yml` и автоматически слушает порты `80`/`443`, проксируя трафик к Grafana.















   - Пара `GRAFANA_DOMAIN` + `TRAEFIK_ACME_EMAIL` (email для Let's Encrypt) используется в переменных `VIRTUAL_HOST`/`LETSENCRYPT_*`; заполните их перед деплоем.















   - Контейнер `nginx-acme` (образ `nginxproxy/acme-companion`) выпускает и обновляет сертификаты через HTTP-01 challenge, поэтому порт `80` должен быть открыт наружу.















   - За пределы сервера попадает только Grafana по `https://<GRAFANA_DOMAIN>`; backend, Loki и Promtail продолжают жить внутри `app-network`.















6. **Проверка.**















   ```bash















   docker compose logs -f promtail















   docker compose logs -f loki















   docker compose logs -f nginx















   docker compose logs -f nginx-acme















   ```















   После появления запросов API в Grafana → `Dashboards` → `Lang Agent / Backend Observability` появятся графики и таблицы.































### Алерты















Уведомления об ошибках:































**Sentry:**















- Автоматический трекинг ошибок















- Stack traces















- Группировка похожих ошибок















- Интеграция с Telegram/Slack































**Telegram bot для алертов:**















- Критические ошибки















- Недоступность сервиса















- Высокая нагрузка































**Email уведомления:**















- Резервный канал для критических алертов































### Метрики















Мониторинг производительности:































**Prometheus + Grafana:**















- CPU, RAM, Disk usage















- Request rate, latency















- Database queries performance















- Active users































**Application metrics:**















- Response time по эндпоинтам















- Количество активных пользователей















- Использование LLM API (tokens, cost)















- Успешность упражнений















- FastAPI exposes `/metrics` via `prometheus_fastapi_instrumentator`, so Prometheus can scrape `http_requests_total`, latency histograms and in-progress gauges without sidecar exporters.















- Custom histogram `app_request_latency_seconds` stores `request_id` as an exemplar value for each observed bucket, making it trivial to pivot from a Grafana chart to Loki logs with the same `request_id`.































**Health checks:**















- `/health` endpoint для проверки доступности















- Database connectivity check















- External APIs availability































## Backup















Резервное копирование БД и данных:































### База данных















**Ежедневные бэкапы:**















```bash















#!/bin/bash















# backup.sh















BACKUP_DIR=/var/backups/postgres















DATE=$(date +%Y%m%d_%H%M%S)















pg_dump -h localhost -U postgres langagent | gzip > $BACKUP_DIR/backup_$DATE.sql.gz































# Удаление старых бэкапов (старше 30 дней)















find $BACKUP_DIR -name "backup_*.sql.gz" -mtime +30 -delete















```































**Cron job:**















```bash















0 2 * * * /usr/local/bin/backup.sh















```































**Копирование во внешнее хранилище:**















```bash















rclone copy $BACKUP_DIR/backup_$DATE.sql.gz object-storage:lang-agent/database/















```































### Файлы пользователей















- Синхронизация с объектным хранилищем (если используется локальное хранение)















- Версионирование в выбранном хранилище















- Регулярная проверка целостности































### Восстановление















```bash















# Восстановление из бэкапа















gunzip < backup_20250109.sql.gz | psql -h localhost -U postgres langagent































# Восстановление из внешнего хранилища















rclone copy object-storage:lang-agent/database/backup_20250109.sql.gz .















gunzip < backup_20250109.sql.gz | psql -h localhost -U postgres langagent















```































## Rollback















Откат на предыдущую версию при проблемах:































### Процедура ручного rollback































1. **SSH на сервер:**















```bash















ssh user@your-server















cd /opt/lang-agent















```































2. **Откатить Docker образ на предыдущую версию:**















```bash















# Посмотреть доступные версии образа















docker images | grep langagent-backend































# Обновить docker-compose.yml с нужной версией (SHA или тегом)















nano docker-compose.yml















# Изменить: image: username/langagent-backend:latest















# На: image: username/langagent-backend:abc1234 (предыдущий SHA)































# Или сделать pull конкретной версии















docker pull username/langagent-backend:abc1234















docker tag username/langagent-backend:abc1234 username/langagent-backend:latest















```































3. **Откатить миграции (если требуется):**















```bash















docker-compose exec backend alembic downgrade -1















```































4. **Перезапустить контейнеры:**















```bash















docker-compose down















docker-compose up -d















```































5. **Проверить работоспособность:**















```bash















# Health check















curl http://localhost:8000/health































# Логи















docker-compose logs -f backend















```































### Автоматический rollback















В CI/CD pipeline настроен автоматический откат при провале health checks после деплоя (см. `docs/ci-cd.md`).































### Восстановление из бэкапа















Если проблемы сохраняются после rollback:















```bash















# Восстановить БД из бэкапа















gunzip < /var/backups/postgres/backup_YYYYMMDD.sql.gz | \















  docker-compose exec -T db psql -U postgres -d langagent















```
### NotificationWorker: streak reminders

- NotificationWorker запускается внутри backend-процесса и активируется только при `NOTIFICATION_WORKER_ENABLED=true`. Локально оставляем флаг выключенным, чтобы не держать соединение с продовой БД.
- `NOTIFICATION_WORKER_INTERVAL_SECONDS` (по умолчанию 1800) задаёт периодичность запусков. Каждый цикл воркер берёт новую AsyncSession, создаёт `NotificationService` и вызывает `process_streak_reminders()`.
- Окно локального времени пользователя регулируется `STREAK_REMINDER_WINDOW_START/END` (значения 0–23). Только в этом окне проверяем активность и создаём записи в `notifications`/`streak_reminders`.
- `STREAK_REMINDER_RETENTION_DAYS` определяет, сколько дней хранится аудит streak_reminders (default 7) — старые строки удаляются в начале цикла.
- Проверить работу можно через `/api/notifications` (или `POST /api/notifications/read-all`) либо напрямую в Postgres. В проде достаточно включить флаг и перезапустить backend — воркер стартует в `on_event("startup")`.

### RateLimitResetWorker: суточные лимиты

- Включается флагом `RATE_LIMIT_WORKER_ENABLED`. На локальных окружениях держим `false`, чтобы не создавать лишних фоновых task'ов.
- `RATE_LIMIT_RESET_HOUR_UTC` и `RATE_LIMIT_RESET_MINUTE_UTC` задают время (UTC), когда воркер очищает Redis-ключи из `ratelimit:action:*`. Новые запросы снова начинают считать дневные лимиты (LLM сообщения, упражнения).
- Воркер использует тот же Redis, что и `CacheClient`, поэтому дополнительных подключений не требуется. Логи появляются с префиксом `app.services.rate_limit_worker`.
